{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def extract_fichier(fichier_html, fichier_xml):\n",
    "    '''extrait l'id du fichier'''\n",
    "    with open(fichier_xml, 'a', encoding=\"utf-8\") as f2:\n",
    "        f2.write(\"<fichier>\") #ouvrir balise\n",
    "        fichier = fichier_html.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        f2.write(fichier)\n",
    "        f2.write(\"</fichier>\\n\")\n",
    "\n",
    "\n",
    "def extract_title(fichier_html, fichier_xml):\n",
    "    '''extrait le nom du fichier, le numero du bulletin et sa date'''\n",
    "    with open(fichier_html, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    balise_title = soup.title.string.split(\">\")\n",
    "\n",
    "    date = balise_title[0].split(\"/\")\n",
    "    fixed_date = date[2][:-1] + \"/\" + date[1] + \"/\" + date[0]\n",
    "\n",
    "\n",
    "    titre = balise_title[2]\n",
    "    num_bulletin = balise_title[1].split(\"\\xa0\")[1]\n",
    "\n",
    "    with open(fichier_xml, 'a', encoding=\"utf-8\") as f2:\n",
    "        f2.write(\"<numero>\") #ouvrir balise\n",
    "        f2.write(num_bulletin)\n",
    "        f2.write(\"</numero>\\n\")\n",
    "\n",
    "        f2.write(\"<date>\") #ouvrir balise\n",
    "        f2.write(fixed_date)\n",
    "        f2.write(\"</date>\\n\")\n",
    "\n",
    "        f2.write(\"<titre>\") #ouvrir balise\n",
    "        f2.write(titre)\n",
    "        f2.write(\"</titre>\\n\")\n",
    "\n",
    "def extract_author(fichier_html, fichier_xml):\n",
    "    '''extrait l'auteur du fichier'''\n",
    "    with open(fichier_html, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    \n",
    "    meta_tag = soup.find(\"meta\", attrs={\"name\": \"author\"})\n",
    "    span_tags = soup.find_all(\"span\", class_=\"style95\")\n",
    "\n",
    "    # Extract and print the content if found\n",
    "    if meta_tag:\n",
    "        #author = meta_tag.get(\"content\")  # Get the value of the \"content\" attribute\n",
    "\n",
    "        with open(fichier_xml, 'a', encoding=\"utf-8\") as f2:\n",
    "            for span in span_tags:\n",
    "                content = span.get_text(separator=\" \", strip=True)  # Extract clean text\n",
    "                if(content != \"\" and \"email\" in content and \"-\" in content):\n",
    "                    f2.write(\"<auteur>\") #ouvrir balise\n",
    "                    nom = content.split(\"-\")[1] + \"-\" + content.split(\"-\")[2]\n",
    "                    f2.write(nom)\n",
    "                    f2.write(\"</auteur>\\n\")\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_title(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/BULLETINS/76516.htm\", \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/test.XML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(fichier_html, fichier_xml):\n",
    "    '''extrait le texte du fichier'''\n",
    "    with open(fichier_html, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    \n",
    "    span_tags = soup.find_all(\"span\", class_=\"style95\")\n",
    "    texte = \"\"\n",
    "    with open(fichier_xml, 'a', encoding=\"utf-8\") as f2:\n",
    "        # Iterate through each <span> and extract text separately\n",
    "        for i, span in enumerate(span_tags):\n",
    "            for content in span.stripped_strings:\n",
    "                if(content != \"\"):\n",
    "                    if(content[-1] == \".\"):\n",
    "                        texte = texte + \" \" + content\n",
    "        f2.write(\"<texte>\") #ouvrir balise\n",
    "        f2.write(texte)\n",
    "        f2.write(\"</texte>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_text(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/BULLETINS/76516.htm\", \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/test.XML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rubrique(fichier_html, fichier_xml):\n",
    "    '''extrait le texte du fichier'''\n",
    "    with open(fichier_html, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    \n",
    "    span_tags = soup.find_all(\"span\", class_=\"style42\")\n",
    "\n",
    "    with open(fichier_xml, 'a', encoding=\"utf-8\") as f2:\n",
    "        # Iterate through each <span> and extract text separately\n",
    "        for i, span in enumerate(span_tags):\n",
    "            for content in span.stripped_strings:\n",
    "                    if( \"/\" not in content):\n",
    "                        f2.write(\"<rubrique>\") #ouvrir balise\n",
    "                        f2.write(content)\n",
    "                        f2.write(\"</rubrique>\\n\")\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_rubrique(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/BULLETINS/76510.htm\", \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/test_bulletin.XML\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_legends(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    images_info = []\n",
    "    \n",
    "    for div in soup.find_all(\"div\", style=\"text-align: center\"):\n",
    "        img_tag = div.find(\"img\")\n",
    "        span_tag = div.find(\"span\", class_=\"style21\")\n",
    "        \n",
    "        if img_tag and span_tag:\n",
    "            img_url = img_tag[\"src\"]\n",
    "            legende = span_tag.get_text(strip=True)\n",
    "            images_info.append((img_url, legende))\n",
    "    return images_info\n",
    "\n",
    "def write_image_infos(fichier_html, fichier_xml):\n",
    "    img_infos = extract_images_legends(fichier_html)\n",
    "    #extract images using .jpg  67940\n",
    "    with open(fichier_xml, 'a', encoding=\"utf-8\") as f2:\n",
    "        if img_infos:\n",
    "            f2.write(\"<images>\\n\")\n",
    "            # Extract and print the 'src' attribute of each image\n",
    "            for (img_src, img_legende) in img_infos:\n",
    "                f2.write(\"<image>\\n\")\n",
    "                f2.write(\"<urlImage>\")\n",
    "                f2.write(img_src)\n",
    "                f2.write(\"</urlImage>\\n\")\n",
    "                    \n",
    "                f2.write(\"<legendeImage>\")\n",
    "                #print(img_caption)\n",
    "                f2.write(img_legende)\n",
    "                f2.write(\"</legendeImage>\\n\")\n",
    "                f2.write(\"</image>\\n\")\n",
    "            f2.write(\"</images>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_image_infos(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/BULLETINS/67940.htm\", \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/test.XML\")\n",
    "#extract_images_legends(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/BULLETINS/67940.htm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_contacts(fichier_html, fichier_xml):\n",
    "    '''Extracts text from spans without links to \"67391.htm\"'''\n",
    "    # Open and parse the HTML file\n",
    "    with open(fichier_html, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    # Find all <span> tags with class \"style85\"\n",
    "    span_tags = soup.find_all(\"span\", class_=\"style85\")\n",
    "\n",
    "    with open(fichier_xml, 'a', encoding=\"utf-8\") as f2:\n",
    "        # Iterate through each <span> tag\n",
    "        for span in span_tags:\n",
    "            content = span.get_text(separator=\" \", strip=True)  # Extract clean text\n",
    "            if(\"-\" in content):\n",
    "                #print(content)  # Debugging output\n",
    "                f2.write(\"<contact>\")  # Open tag\n",
    "                f2.write(content)  # Write extracted text\n",
    "                f2.write(\"</contact>\\n\")  # Close tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_contacts(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/BULLETINS/76508.htm\", \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/test_bulletin.XML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every individual bulletin\n",
    "def extract_file(fichier_html, fichier_xml):\n",
    "    with open(fichier_xml, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"<bulletin>\\n\")\n",
    "    extract_fichier(fichier_html, fichier_xml)\n",
    "    extract_title(fichier_html, fichier_xml)\n",
    "    extract_rubrique(fichier_html, fichier_xml)\n",
    "    extract_author(fichier_html, fichier_xml)\n",
    "    extract_text(fichier_html, fichier_xml)\n",
    "    write_image_infos(fichier_html, fichier_xml)\n",
    "    extract_contacts(fichier_html, fichier_xml)\n",
    "    with open(fichier_xml, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"</bulletin>\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_file(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/BULLETINS/70914.htm\", \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/one_bulletin.XML\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all rubriques\n",
    "from pathlib import Path\n",
    "def extract_all_files(directory, fichier_xml):\n",
    "    with open(fichier_xml, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"<corpus>\\n\")\n",
    "    \n",
    "    dir_path = Path(directory)\n",
    "    \n",
    "    if not dir_path.is_dir():\n",
    "        print(\"Invalid directory path\")\n",
    "        return\n",
    "    \n",
    "    #for each htm file in the repository \n",
    "    for file in dir_path.iterdir():  # Iterate through directory contents\n",
    "        if file.is_file():  # Ensure it's a file\n",
    "            file_name = str(file.resolve())  # Print full absolute path\n",
    "        extract_file(fichier_html= file_name, fichier_xml= fichier_xml)\n",
    "\n",
    "    with open(fichier_xml, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"</corpus>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_all_files(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/BULLETINS\", \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD2/corpus.XML\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
