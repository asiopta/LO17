{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ntich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')  # Télécharge les données de base (dont Snowball)\n",
    "nltk.download('stopwords')  # Pour les stopwords français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'abord',\n",
       " 'afin',\n",
       " 'ah',\n",
       " 'ai',\n",
       " 'aie',\n",
       " 'ainsi',\n",
       " 'ait',\n",
       " 'allaient',\n",
       " 'allons',\n",
       " 'alors',\n",
       " 'anterieur',\n",
       " 'anterieure',\n",
       " 'anterieures',\n",
       " 'antérieur',\n",
       " 'antérieure',\n",
       " 'antérieures',\n",
       " 'apres',\n",
       " 'après',\n",
       " 'as',\n",
       " 'assez',\n",
       " 'attendu',\n",
       " 'au',\n",
       " 'aupres',\n",
       " 'auquel',\n",
       " 'aura',\n",
       " 'auraient',\n",
       " 'aurait',\n",
       " 'auront',\n",
       " 'aussi',\n",
       " 'autre',\n",
       " 'autrement',\n",
       " 'autres',\n",
       " 'autrui',\n",
       " 'aux',\n",
       " 'auxquelles',\n",
       " 'auxquels',\n",
       " 'avaient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avant',\n",
       " 'avec',\n",
       " 'avoir',\n",
       " 'avons',\n",
       " 'ayant',\n",
       " 'bas',\n",
       " 'basee',\n",
       " 'bat',\n",
       " \"c'\",\n",
       " 'car',\n",
       " 'ce',\n",
       " 'ceci',\n",
       " 'cela',\n",
       " 'celle',\n",
       " 'celle-ci',\n",
       " 'celle-la',\n",
       " 'celle-là',\n",
       " 'celles',\n",
       " 'celles-ci',\n",
       " 'celles-la',\n",
       " 'celles-là',\n",
       " 'celui',\n",
       " 'celui-ci',\n",
       " 'celui-la',\n",
       " 'celui-là',\n",
       " 'cent',\n",
       " 'cependant',\n",
       " 'certain',\n",
       " 'certaine',\n",
       " 'certaines',\n",
       " 'certains',\n",
       " 'certes',\n",
       " 'ces',\n",
       " 'cet',\n",
       " 'cette',\n",
       " 'ceux',\n",
       " 'ceux-ci',\n",
       " 'ceux-là',\n",
       " 'chacun',\n",
       " 'chacune',\n",
       " 'chaque',\n",
       " 'chez',\n",
       " 'ci',\n",
       " 'cinq',\n",
       " 'cinquantaine',\n",
       " 'cinquante',\n",
       " 'cinquantième',\n",
       " 'cinquième',\n",
       " 'combien',\n",
       " 'comme',\n",
       " 'comment',\n",
       " 'compris',\n",
       " 'concernant',\n",
       " 'c’',\n",
       " \"d'\",\n",
       " 'da',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'debout',\n",
       " 'dedans',\n",
       " 'dehors',\n",
       " 'deja',\n",
       " 'dejà',\n",
       " 'delà',\n",
       " 'depuis',\n",
       " 'derriere',\n",
       " 'derrière',\n",
       " 'des',\n",
       " 'desormais',\n",
       " 'desquelles',\n",
       " 'desquels',\n",
       " 'dessous',\n",
       " 'dessus',\n",
       " 'deux',\n",
       " 'deuxième',\n",
       " 'deuxièmement',\n",
       " 'devant',\n",
       " 'devers',\n",
       " 'devra',\n",
       " 'different',\n",
       " 'differente',\n",
       " 'differentes',\n",
       " 'differents',\n",
       " 'différent',\n",
       " 'différente',\n",
       " 'différentes',\n",
       " 'différents',\n",
       " 'dire',\n",
       " 'directe',\n",
       " 'directement',\n",
       " 'dit',\n",
       " 'dite',\n",
       " 'dits',\n",
       " 'divers',\n",
       " 'diverse',\n",
       " 'diverses',\n",
       " 'dix',\n",
       " 'dix-huit',\n",
       " 'dix-neuf',\n",
       " 'dix-sept',\n",
       " 'dixième',\n",
       " 'doit',\n",
       " 'doivent',\n",
       " 'donc',\n",
       " 'dont',\n",
       " 'douze',\n",
       " 'douzième',\n",
       " 'du',\n",
       " 'duquel',\n",
       " 'durant',\n",
       " 'dès',\n",
       " 'déja',\n",
       " 'déjà',\n",
       " 'désormais',\n",
       " 'd’',\n",
       " 'effet',\n",
       " 'egalement',\n",
       " 'eh',\n",
       " 'elle',\n",
       " 'elle-meme',\n",
       " 'elle-même',\n",
       " 'elles',\n",
       " 'elles-memes',\n",
       " 'elles-mêmes',\n",
       " 'en',\n",
       " 'encore',\n",
       " 'enfin',\n",
       " 'entre',\n",
       " 'envers',\n",
       " 'environ',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'etaient',\n",
       " 'etais',\n",
       " 'etait',\n",
       " 'etant',\n",
       " 'etc',\n",
       " 'etre',\n",
       " 'eu',\n",
       " 'eux',\n",
       " 'eux-mêmes',\n",
       " 'exactement',\n",
       " 'excepté',\n",
       " 'facon',\n",
       " 'fais',\n",
       " 'faisaient',\n",
       " 'faisant',\n",
       " 'fait',\n",
       " 'façon',\n",
       " 'feront',\n",
       " 'font',\n",
       " 'gens',\n",
       " 'ha',\n",
       " 'hem',\n",
       " 'hep',\n",
       " 'hi',\n",
       " 'ho',\n",
       " 'hormis',\n",
       " 'hors',\n",
       " 'hou',\n",
       " 'houp',\n",
       " 'hue',\n",
       " 'hui',\n",
       " 'huit',\n",
       " 'huitième',\n",
       " 'hé',\n",
       " 'i',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'importe',\n",
       " \"j'\",\n",
       " 'je',\n",
       " 'jusqu',\n",
       " 'jusque',\n",
       " 'juste',\n",
       " 'j’',\n",
       " \"l'\",\n",
       " 'la',\n",
       " 'laisser',\n",
       " 'laquelle',\n",
       " 'le',\n",
       " 'lequel',\n",
       " 'les',\n",
       " 'lesquelles',\n",
       " 'lesquels',\n",
       " 'leur',\n",
       " 'leurs',\n",
       " 'longtemps',\n",
       " 'lors',\n",
       " 'lorsque',\n",
       " 'lui',\n",
       " 'lui-meme',\n",
       " 'lui-même',\n",
       " 'là',\n",
       " 'lès',\n",
       " 'l’',\n",
       " \"m'\",\n",
       " 'ma',\n",
       " 'maint',\n",
       " 'maintenant',\n",
       " 'mais',\n",
       " 'malgre',\n",
       " 'malgré',\n",
       " 'me',\n",
       " 'meme',\n",
       " 'memes',\n",
       " 'merci',\n",
       " 'mes',\n",
       " 'mien',\n",
       " 'mienne',\n",
       " 'miennes',\n",
       " 'miens',\n",
       " 'mille',\n",
       " 'moi',\n",
       " 'moi-meme',\n",
       " 'moi-même',\n",
       " 'moindres',\n",
       " 'moins',\n",
       " 'mon',\n",
       " 'même',\n",
       " 'mêmes',\n",
       " 'm’',\n",
       " \"n'\",\n",
       " 'na',\n",
       " 'ne',\n",
       " 'neanmoins',\n",
       " 'neuvième',\n",
       " 'ni',\n",
       " 'nombreuses',\n",
       " 'nombreux',\n",
       " 'nos',\n",
       " 'notamment',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'nous-mêmes',\n",
       " 'nouveau',\n",
       " 'nul',\n",
       " 'néanmoins',\n",
       " 'nôtre',\n",
       " 'nôtres',\n",
       " 'n’',\n",
       " 'o',\n",
       " 'on',\n",
       " 'ont',\n",
       " 'onze',\n",
       " 'onzième',\n",
       " 'or',\n",
       " 'ou',\n",
       " 'ouias',\n",
       " 'ouste',\n",
       " 'outre',\n",
       " 'ouvert',\n",
       " 'ouverte',\n",
       " 'ouverts',\n",
       " 'où',\n",
       " 'par',\n",
       " 'parce',\n",
       " 'parfois',\n",
       " 'parle',\n",
       " 'parlent',\n",
       " 'parler',\n",
       " 'parmi',\n",
       " 'partant',\n",
       " 'pas',\n",
       " 'pendant',\n",
       " 'pense',\n",
       " 'permet',\n",
       " 'personne',\n",
       " 'peu',\n",
       " 'peut',\n",
       " 'peuvent',\n",
       " 'peux',\n",
       " 'plus',\n",
       " 'plusieurs',\n",
       " 'plutot',\n",
       " 'plutôt',\n",
       " 'possible',\n",
       " 'possibles',\n",
       " 'pour',\n",
       " 'pourquoi',\n",
       " 'pourrais',\n",
       " 'pourrait',\n",
       " 'pouvait',\n",
       " 'prealable',\n",
       " 'precisement',\n",
       " 'premier',\n",
       " 'première',\n",
       " 'premièrement',\n",
       " 'pres',\n",
       " 'procedant',\n",
       " 'proche',\n",
       " 'près',\n",
       " 'préalable',\n",
       " 'précisement',\n",
       " 'pu',\n",
       " 'puis',\n",
       " 'puisque',\n",
       " \"qu'\",\n",
       " 'quand',\n",
       " 'quant',\n",
       " 'quant-à-soi',\n",
       " 'quarante',\n",
       " 'quatorze',\n",
       " 'quatre',\n",
       " 'quatre-vingt',\n",
       " 'quatrième',\n",
       " 'quatrièmement',\n",
       " 'que',\n",
       " 'quel',\n",
       " 'quelconque',\n",
       " 'quelle',\n",
       " 'quelles',\n",
       " \"quelqu'un\",\n",
       " 'quelque',\n",
       " 'quelques',\n",
       " 'quels',\n",
       " 'qui',\n",
       " 'quiconque',\n",
       " 'quinze',\n",
       " 'quoi',\n",
       " 'quoique',\n",
       " 'qu’',\n",
       " 'relative',\n",
       " 'relativement',\n",
       " 'rend',\n",
       " 'rendre',\n",
       " 'restant',\n",
       " 'reste',\n",
       " 'restent',\n",
       " 'retour',\n",
       " 'revoici',\n",
       " 'revoila',\n",
       " 'revoilà',\n",
       " \"s'\",\n",
       " 'sa',\n",
       " 'sait',\n",
       " 'sans',\n",
       " 'sauf',\n",
       " 'se',\n",
       " 'seize',\n",
       " 'selon',\n",
       " 'semblable',\n",
       " 'semblaient',\n",
       " 'semble',\n",
       " 'semblent',\n",
       " 'sent',\n",
       " 'sept',\n",
       " 'septième',\n",
       " 'sera',\n",
       " 'seraient',\n",
       " 'serait',\n",
       " 'seront',\n",
       " 'ses',\n",
       " 'seul',\n",
       " 'seule',\n",
       " 'seulement',\n",
       " 'seules',\n",
       " 'seuls',\n",
       " 'si',\n",
       " 'sien',\n",
       " 'sienne',\n",
       " 'siennes',\n",
       " 'siens',\n",
       " 'sinon',\n",
       " 'six',\n",
       " 'sixième',\n",
       " 'soi',\n",
       " 'soi-meme',\n",
       " 'soi-même',\n",
       " 'soit',\n",
       " 'soixante',\n",
       " 'son',\n",
       " 'sont',\n",
       " 'sous',\n",
       " 'souvent',\n",
       " 'specifique',\n",
       " 'specifiques',\n",
       " 'spécifique',\n",
       " 'spécifiques',\n",
       " 'stop',\n",
       " 'suffisant',\n",
       " 'suffisante',\n",
       " 'suffit',\n",
       " 'suis',\n",
       " 'suit',\n",
       " 'suivant',\n",
       " 'suivante',\n",
       " 'suivantes',\n",
       " 'suivants',\n",
       " 'suivre',\n",
       " 'sur',\n",
       " 'surtout',\n",
       " 's’',\n",
       " \"t'\",\n",
       " 'ta',\n",
       " 'tant',\n",
       " 'te',\n",
       " 'tel',\n",
       " 'telle',\n",
       " 'tellement',\n",
       " 'telles',\n",
       " 'tels',\n",
       " 'tenant',\n",
       " 'tend',\n",
       " 'tenir',\n",
       " 'tente',\n",
       " 'tes',\n",
       " 'tien',\n",
       " 'tienne',\n",
       " 'tiennes',\n",
       " 'tiens',\n",
       " 'toi',\n",
       " 'toi-meme',\n",
       " 'toi-même',\n",
       " 'ton',\n",
       " 'touchant',\n",
       " 'toujours',\n",
       " 'tous',\n",
       " 'tout',\n",
       " 'toute',\n",
       " 'toutes',\n",
       " 'treize',\n",
       " 'trente',\n",
       " 'tres',\n",
       " 'trois',\n",
       " 'troisième',\n",
       " 'troisièmement',\n",
       " 'très',\n",
       " 'tu',\n",
       " 'té',\n",
       " 't’',\n",
       " 'un',\n",
       " 'une',\n",
       " 'unes',\n",
       " 'uns',\n",
       " 'va',\n",
       " 'vais',\n",
       " 'vas',\n",
       " 'vers',\n",
       " 'via',\n",
       " 'vingt',\n",
       " 'voici',\n",
       " 'voila',\n",
       " 'voilà',\n",
       " 'vont',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'votres',\n",
       " 'vous',\n",
       " 'vous-mêmes',\n",
       " 'vu',\n",
       " 'vé',\n",
       " 'vôtre',\n",
       " 'vôtres',\n",
       " 'y',\n",
       " 'à',\n",
       " 'â',\n",
       " 'ça',\n",
       " 'ès',\n",
       " 'également',\n",
       " 'étaient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étant',\n",
       " 'être',\n",
       " 'ô'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "# la liste fr-stop contient les stop words en français\n",
    "fr_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('la', 'le')\n",
      "('portée', 'portée')\n",
      "('du', 'de')\n",
      "('signal', 'signal')\n",
      "('est', 'être')\n",
      "('très', 'très')\n",
      "('faible', 'faible')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "texte = \"la portée du signal est très faible.\"\n",
    "doc = nlp(texte)\n",
    "\n",
    "for token in doc:\n",
    "    if token in fr_stop:\n",
    "        pass\n",
    "    else:\n",
    "        print((token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lemmas(corpus, output):\n",
    "    with open(output, \"a\", encoding=\"utf-8\") as f_w:\n",
    "        f_w.write(\"mot\" + \"\\t\" + \"lemme\\n\")\n",
    "        with open(corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "            for ligne in f.readlines():\n",
    "                contenu = str(ligne)\n",
    "                if \"<texte>\" in ligne:\n",
    "                    #print(\"texte ok\")\n",
    "                    contenu = contenu.replace(\"<texte>\", \"\")\n",
    "                    contenu = contenu.replace(\"</texte>\", \"\")\n",
    "                    #print(contenu)\n",
    "                    \n",
    "                elif \"<titre>\" in ligne:\n",
    "                    #print(\"titre ok\")\n",
    "                    contenu = contenu.replace(\"<titre>\", \"\")\n",
    "                    contenu = contenu.replace(\"</titre>\", \"\")\n",
    "                    #print(contenu)\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                nlp_content = nlp(contenu)\n",
    "                for token in nlp_content:\n",
    "                    if (token in fr_stop) or (token == \"\\\\s+\") or (token==\"\\n\") or (token==\"\\t\") or (token == \" \"):\n",
    "                        pass\n",
    "                    else:\n",
    "                        to_write = token.text + \"\\t\" + token.lemma_ + \"\\n\"\n",
    "                        if to_write.lower() != to_write.upper():\n",
    "                            f_w.write(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lemmas(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD3/corpus_nettoyé2.XML\",\n",
    "               \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/lemmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\") # Choose a language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def create_stems(corpus, output):\n",
    "    \n",
    "    with open(output, \"a\", encoding=\"utf-8\") as f_w:\n",
    "        f_w.write(\"mot\" + \"\\t\" + \"racine\\n\")\n",
    "        with open(corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "            for ligne in f.readlines():\n",
    "                contenu = str(ligne)\n",
    "                if \"<texte>\" in ligne:\n",
    "                    #print(\"texte ok\")\n",
    "                    contenu = contenu.replace(\"<texte>\", \"\")\n",
    "                    contenu = contenu.replace(\"</texte>\", \"\")\n",
    "                    #print(contenu)\n",
    "                    \n",
    "                elif \"<titre>\" in ligne:\n",
    "                    #print(\"titre ok\")\n",
    "                    contenu = contenu.replace(\"<titre>\", \"\")\n",
    "                    contenu = contenu.replace(\"</titre>\", \"\")\n",
    "                    #print(contenu)\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                nlp_content = nlp(contenu)\n",
    "                for token in nlp_content:\n",
    "                    if (token in fr_stop) or (token == \"\\\\s+\") or (token==\"\\n\") or (token==\"\\t\") or (token == \" \"):\n",
    "                        pass\n",
    "                    else:\n",
    "                        to_write = token.text + \"\\t\" + stemmer.stem(token.text) + \"\\n\"\n",
    "                        if to_write.lower() != to_write.upper():\n",
    "                            f_w.write(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_stems(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD3/corpus_nettoyé2.XML\",\n",
    "               \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/stems.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parfois stem des noms propres qu'on veut pas forcément changer / on souhaite garder tels qu'ils sont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On prefere continuer avec les lemmes, puisque c'est plus fiable. On perd moins le sens des mots, en comparant avec les racines (mots propes, etc), meme si cela nécessite plus de ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11109"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_unique_lemmas(file_path):\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")  # Lecture du fichier CSV (TSV)\n",
    "    \n",
    "    unique_lemmas = df[\"lemme\"].nunique()  # Compte le nombre de lemmes uniques\n",
    "    \n",
    "    return unique_lemmas\n",
    "\n",
    "count_unique_lemmas(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/lemmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8496"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_unique_lemmas(file_path):\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")  # Lecture du fichier CSV (TSV)\n",
    "    \n",
    "    unique_lemmas = df[\"racine\"].nunique()  # Compte le nombre de lemmes uniques\n",
    "    \n",
    "    return unique_lemmas\n",
    "\n",
    "count_unique_lemmas(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/stems.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
