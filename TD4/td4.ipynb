{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('popular')  # Télécharge les données de base (dont Snowball)\n",
    "#nltk.download('stopwords')  # Pour les stopwords français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "# la liste fr-stop contient les stop words en français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('la', 'le')\n",
      "('portée', 'portée')\n",
      "('du', 'de')\n",
      "('signal', 'signal')\n",
      "('est', 'être')\n",
      "('très', 'très')\n",
      "('faible', 'faible')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "texte = \"la portée du signal est très faible.\"\n",
    "doc = nlp(texte)\n",
    "\n",
    "for token in doc:\n",
    "    if token in fr_stop:\n",
    "        pass\n",
    "    else:\n",
    "        print((token.text, token.lemma_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lemmas(corpus, output):\n",
    "    with open(output, \"a\", encoding=\"utf-8\") as f_w:\n",
    "        f_w.write(\"mot\" + \"\\t\" + \"lemme\\n\")\n",
    "        with open(corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "            for ligne in f.readlines():\n",
    "                contenu = str(ligne)\n",
    "                if \"<texte>\" in ligne:\n",
    "                    #print(\"texte ok\")\n",
    "                    contenu = contenu.replace(\"<texte>\", \"\")\n",
    "                    contenu = contenu.replace(\"</texte>\", \"\")\n",
    "                    #print(contenu)\n",
    "                    \n",
    "                elif \"<titre>\" in ligne:\n",
    "                    #print(\"titre ok\")\n",
    "                    contenu = contenu.replace(\"<titre>\", \"\")\n",
    "                    contenu = contenu.replace(\"</titre>\", \"\")\n",
    "                    #print(contenu)\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                nlp_content = nlp(contenu)\n",
    "                for token in nlp_content:\n",
    "                    if (token in fr_stop) or (token == \"\\\\s+\") or (token==\"\\n\") or (token==\"\\t\") or (token == \" \"):\n",
    "                        pass\n",
    "                    else:\n",
    "                        to_write = token.text + \"\\t\" + token.lemma_ + \"\\n\"\n",
    "                        if to_write.lower() != to_write.upper():\n",
    "                            f_w.write(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lemmas(\"../TD3/corpus_nettoyé2.XML\",\n",
    "               \"./lemmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\") # Choose a language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def create_stems(corpus, output):\n",
    "    \n",
    "    with open(output, \"a\", encoding=\"utf-8\") as f_w:\n",
    "        f_w.write(\"mot\" + \"\\t\" + \"racine\\n\")\n",
    "        with open(corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "            for ligne in f.readlines():\n",
    "                contenu = str(ligne)\n",
    "                if \"<texte>\" in ligne:\n",
    "                    #print(\"texte ok\")\n",
    "                    contenu = contenu.replace(\"<texte>\", \"\")\n",
    "                    contenu = contenu.replace(\"</texte>\", \"\")\n",
    "                    #print(contenu)\n",
    "                    \n",
    "                elif \"<titre>\" in ligne:\n",
    "                    #print(\"titre ok\")\n",
    "                    contenu = contenu.replace(\"<titre>\", \"\")\n",
    "                    contenu = contenu.replace(\"</titre>\", \"\")\n",
    "                    #print(contenu)\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                nlp_content = nlp(contenu)\n",
    "                for token in nlp_content:\n",
    "                    if (token in fr_stop) or (token == \"\\\\s+\") or (token==\"\\n\") or (token==\"\\t\") or (token == \" \"):\n",
    "                        pass\n",
    "                    else:\n",
    "                        to_write = token.text + \"\\t\" + stemmer.stem(token.text) + \"\\n\"\n",
    "                        if to_write.lower() != to_write.upper():\n",
    "                            f_w.write(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_stems(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD3/corpus_nettoyé2.XML\",\n",
    "               \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/stems.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parfois stem des noms propres qu'on veut pas forcément changer / on souhaite garder tels qu'ils sont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On prefere continuer avec les lemmes, puisque c'est plus fiable. On perd moins le sens des mots, en comparant avec les racines (mots propes, etc), meme si cela nécessite plus de ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11109"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_unique_lemmas(file_path):\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")  # Lecture du fichier CSV (TSV)\n",
    "    \n",
    "    unique_lemmas = df[\"lemme\"].nunique()  # Compte le nombre de lemmes uniques\n",
    "    \n",
    "    return unique_lemmas\n",
    "\n",
    "count_unique_lemmas(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/lemmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8496"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_unique_lemmas(file_path):\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")  # Lecture du fichier CSV (TSV)\n",
    "    \n",
    "    unique_lemmas = df[\"racine\"].nunique()  # Compte le nombre de lemmes uniques\n",
    "    \n",
    "    return unique_lemmas\n",
    "\n",
    "count_unique_lemmas(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/stems.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# %%\n",
    "def calculate_nlp_stats(lemmas_path, stems_path, output_comparison=None):\n",
    "    \"\"\"\n",
    "    Calcule et affiche des statistiques comparatives sur les fichiers de lemmes et de racines.\n",
    "    \n",
    "    Args:\n",
    "        lemmas_path: Chemin vers le fichier CSV des lemmes\n",
    "        stems_path: Chemin vers le fichier CSV des racines\n",
    "        output_comparison: Chemin pour enregistrer un fichier de comparaison (optionnel)\n",
    "    \"\"\"\n",
    "    # Charger les fichiers\n",
    "    lemmas_df = pd.read_csv(lemmas_path, sep='\\t')\n",
    "    stems_df = pd.read_csv(stems_path, sep='\\t')\n",
    "    \n",
    "    # Nettoyer les données (éliminer lignes vides ou invalides)\n",
    "    lemmas_df = lemmas_df.dropna()\n",
    "    stems_df = stems_df.dropna()\n",
    "    \n",
    "    # 1. Statistiques générales\n",
    "    print(\"===== STATISTIQUES GÉNÉRALES =====\")\n",
    "    print(f\"Nombre total de mots analysés (lemmes): {len(lemmas_df)}\")\n",
    "    print(f\"Nombre total de mots analysés (stems): {len(stems_df)}\")\n",
    "    \n",
    "    # 2. Nombre de formes uniques\n",
    "    unique_words_lemmas = lemmas_df['mot'].nunique()\n",
    "    unique_lemmas = lemmas_df['lemme'].nunique()\n",
    "    unique_words_stems = stems_df['mot'].nunique()\n",
    "    unique_stems = stems_df['racine'].nunique()\n",
    "    \n",
    "    print(\"\\n===== FORMES UNIQUES =====\")\n",
    "    print(f\"Mots uniques (lemmes): {unique_words_lemmas}\")\n",
    "    print(f\"Lemmes uniques: {unique_lemmas}\")\n",
    "    print(f\"Mots uniques (stems): {unique_words_stems}\")\n",
    "    print(f\"Racines uniques: {unique_stems}\")\n",
    "    \n",
    "    # 3. Taux de réduction\n",
    "    reduction_rate_lemmas = (1 - unique_lemmas / unique_words_lemmas) * 100\n",
    "    reduction_rate_stems = (1 - unique_stems / unique_words_stems) * 100\n",
    "    \n",
    "    print(\"\\n===== TAUX DE RÉDUCTION =====\")\n",
    "    print(f\"Taux de réduction par lemmatisation: {reduction_rate_lemmas:.2f}%\")\n",
    "    print(f\"Taux de réduction par stemming: {reduction_rate_stems:.2f}%\")\n",
    "    \n",
    "    # 4. Distribution des fréquences\n",
    "    lemma_counts = lemmas_df['lemme'].value_counts()\n",
    "    stem_counts = stems_df['racine'].value_counts()\n",
    "    \n",
    "    print(\"\\n===== DISTRIBUTION DES FRÉQUENCES =====\")\n",
    "    print(\"Top 10 lemmes les plus fréquents:\")\n",
    "    print(lemma_counts.head(10))\n",
    "    \n",
    "    print(\"\\nTop 10 racines les plus fréquentes:\")\n",
    "    print(stem_counts.head(10))\n",
    "    \n",
    "    # 5. Exemples de mappings\n",
    "    print(\"\\n===== EXEMPLES DE MAPPINGS =====\")\n",
    "    print(\"Exemples de mappings mot -> lemme:\")\n",
    "    mapping_examples_lemmas = lemmas_df.groupby('lemme')['mot'].unique().reset_index()\n",
    "    mapping_examples_lemmas['mot_count'] = mapping_examples_lemmas['mot'].apply(len)\n",
    "    mapping_examples_lemmas = mapping_examples_lemmas.sort_values('mot_count', ascending=False).head(10)\n",
    "    \n",
    "    for _, row in mapping_examples_lemmas.iterrows():\n",
    "        print(f\"{row['lemme']}: {', '.join(row['mot'][:5])}{'...' if len(row['mot']) > 5 else ''}\")\n",
    "    \n",
    "    print(\"\\nExemples de mappings mot -> racine:\")\n",
    "    mapping_examples_stems = stems_df.groupby('racine')['mot'].unique().reset_index()\n",
    "    mapping_examples_stems['mot_count'] = mapping_examples_stems['mot'].apply(len)\n",
    "    mapping_examples_stems = mapping_examples_stems.sort_values('mot_count', ascending=False).head(10)\n",
    "    \n",
    "    for _, row in mapping_examples_stems.iterrows():\n",
    "        print(f\"{row['racine']}: {', '.join(row['mot'][:5])}{'...' if len(row['mot']) > 5 else ''}\")\n",
    "    \n",
    "    # 6. Visualisation de la distribution des fréquences\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Distribution des lemmes\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(lemma_counts.head(50), bins=20)\n",
    "    plt.title('Distribution des fréquences des lemmes')\n",
    "    plt.xlabel('Fréquence')\n",
    "    plt.ylabel('Nombre de lemmes')\n",
    "    \n",
    "    # Distribution des stems\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(stem_counts.head(50), bins=20)\n",
    "    plt.title('Distribution des fréquences des stems')\n",
    "    plt.xlabel('Fréquence')\n",
    "    plt.ylabel('Nombre de stems')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 7. Comparer lemmes et racines (pour les mêmes mots)\n",
    "    # Fusionner les DataFrames sur la colonne 'mot'\n",
    "    merged_df = pd.merge(lemmas_df, stems_df, on='mot', how='inner')\n",
    "    unique_transformations = merged_df.groupby(['lemme', 'racine']).size().reset_index(name='count')\n",
    "    \n",
    "    print(\"\\n===== COMPARAISON LEMMES VS STEMS =====\")\n",
    "    print(f\"Nombre de paires uniques (lemme, racine): {len(unique_transformations)}\")\n",
    "    print(\"Exemples où lemme ≠ racine:\")\n",
    "    \n",
    "    diff_examples = merged_df[merged_df['lemme'] != merged_df['racine']].drop_duplicates(['lemme', 'racine']).head(10)\n",
    "    for _, row in diff_examples.iterrows():\n",
    "        print(f\"Mot: {row['mot']} → Lemme: {row['lemme']} → Racine: {row['racine']}\")\n",
    "    \n",
    "    # 8. Sauvegarder la comparaison si requis\n",
    "    if output_comparison:\n",
    "        with open(output_comparison, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Comparaison Lemmatisation vs Stemming\\n\\n\")\n",
    "            f.write(\"## Statistiques générales\\n\")\n",
    "            f.write(f\"- Mots analysés: {len(lemmas_df)}\\n\")\n",
    "            f.write(f\"- Mots uniques: {unique_words_lemmas}\\n\")\n",
    "            f.write(f\"- Lemmes uniques: {unique_lemmas}\\n\")\n",
    "            f.write(f\"- Racines uniques: {unique_stems}\\n\")\n",
    "            f.write(f\"- Taux de réduction (lemmatisation): {reduction_rate_lemmas:.2f}%\\n\")\n",
    "            f.write(f\"- Taux de réduction (stemming): {reduction_rate_stems:.2f}%\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Conclusions\\n\")\n",
    "            if reduction_rate_stems > reduction_rate_lemmas:\n",
    "                f.write(\"- Le stemming permet une réduction plus importante du vocabulaire.\\n\")\n",
    "            else:\n",
    "                f.write(\"- La lemmatisation permet une réduction plus importante du vocabulaire.\\n\")\n",
    "            \n",
    "            f.write(\"- La lemmatisation préserve mieux le sens linguistique des mots.\\n\")\n",
    "            f.write(\"- Le stemming est généralement plus rapide mais moins précis linguistiquement.\\n\")\n",
    "        print(f\"\\nComparaison sauvegardée dans {output_comparison}\")\n",
    "\n",
    "# %%\n",
    "# Exécuter l'analyse\n",
    "calculate_nlp_stats(\n",
    "    lemmas_path=\"./lemmes.csv\",\n",
    "    stems_path=\"./stems.csv\",\n",
    "    output_comparison=\"./comparaison_lemmes_stems.md\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Analyse supplémentaire : longueur des lemmes vs longueur des stems\n",
    "def compare_length_reduction():\n",
    "    \"\"\"Compare la réduction de longueur entre mots originaux, lemmes et stems\"\"\"\n",
    "    lemmas_df = pd.read_csv(\"./lemmes.csv\", sep='\\t')\n",
    "    stems_df = pd.read_csv(\"./stems.csv\", sep='\\t')\n",
    "    \n",
    "    # Calculer les longueurs\n",
    "    lemmas_df['mot_len'] = lemmas_df['mot'].apply(len)\n",
    "    lemmas_df['lemme_len'] = lemmas_df['lemme'].apply(len)\n",
    "    lemmas_df['len_diff'] = lemmas_df['mot_len'] - lemmas_df['lemme_len']\n",
    "    \n",
    "    stems_df['mot_len'] = stems_df['mot'].apply(len)\n",
    "    stems_df['stem_len'] = stems_df['racine'].apply(len)\n",
    "    stems_df['len_diff'] = stems_df['mot_len'] - stems_df['stem_len']\n",
    "    \n",
    "    # Afficher les statistiques\n",
    "    print(\"===== RÉDUCTION DE LONGUEUR =====\")\n",
    "    print(\"Lemmatisation:\")\n",
    "    print(f\"Longueur moyenne des mots: {lemmas_df['mot_len'].mean():.2f} caractères\")\n",
    "    print(f\"Longueur moyenne des lemmes: {lemmas_df['lemme_len'].mean():.2f} caractères\")\n",
    "    print(f\"Réduction moyenne: {lemmas_df['len_diff'].mean():.2f} caractères\")\n",
    "    \n",
    "    print(\"\\nStemming:\")\n",
    "    print(f\"Longueur moyenne des mots: {stems_df['mot_len'].mean():.2f} caractères\")\n",
    "    print(f\"Longueur moyenne des stems: {stems_df['stem_len'].mean():.2f} caractères\")\n",
    "    print(f\"Réduction moyenne: {stems_df['len_diff'].mean():.2f} caractères\")\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(lemmas_df['mot_len'], lemmas_df['lemme_len'], alpha=0.1)\n",
    "    plt.plot([0, 20], [0, 20], 'r--')  # Ligne diagonale x=y\n",
    "    plt.xlabel('Longueur du mot original')\n",
    "    plt.ylabel('Longueur du lemme')\n",
    "    plt.title('Mot vs Lemme')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(stems_df['mot_len'], stems_df['stem_len'], alpha=0.1)\n",
    "    plt.plot([0, 20], [0, 20], 'r--')  # Ligne diagonale x=y\n",
    "    plt.xlabel('Longueur du mot original')\n",
    "    plt.ylabel('Longueur du stem')\n",
    "    plt.title('Mot vs Stem')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "compare_length_reduction()\n",
    "\n",
    "# %%\n",
    "# Analyse de la pertinence linguistique\n",
    "def analyze_linguistic_relevance():\n",
    "    \"\"\"Analyse la pertinence linguistique en comparant des exemples de lemmes et stems\"\"\"\n",
    "    lemmas_df = pd.read_csv(\"./lemmes.csv\", sep='\\t')\n",
    "    stems_df = pd.read_csv(\"./stems.csv\", sep='\\t')\n",
    "    \n",
    "    # Fusionner pour comparer\n",
    "    merged = pd.merge(lemmas_df, stems_df, on='mot', how='inner')\n",
    "    \n",
    "    # Exemples où lemme et stem diffèrent significativement\n",
    "    significant_diff = merged[merged['lemme'] != merged['racine']]\n",
    "    \n",
    "    # Sélectionner quelques exemples intéressants (verbes, adjectifs, etc.)\n",
    "    print(\"===== ANALYSE LINGUISTIQUE =====\")\n",
    "    print(\"Exemples où lemmatisation et stemming donnent des résultats différents:\")\n",
    "    \n",
    "    # Prendre 15 exemples aléatoires\n",
    "    samples = significant_diff.sample(min(15, len(significant_diff)))\n",
    "    for _, row in samples.iterrows():\n",
    "        print(f\"Mot: {row['mot']} → Lemme: {row['lemme']} → Stem: {row['racine']}\")\n",
    "    \n",
    "    # Cas particuliers des verbes\n",
    "    verbs = merged[merged['lemme'].str.endswith('er') | merged['lemme'].str.endswith('ir') | merged['lemme'].str.endswith('oir') | merged['lemme'].str.endswith('re')]\n",
    "    \n",
    "    print(\"\\nCas des verbes:\")\n",
    "    for _, row in verbs.sample(min(10, len(verbs))).iterrows():\n",
    "        print(f\"Mot: {row['mot']} → Lemme: {row['lemme']} → Stem: {row['racine']}\")\n",
    "\n",
    "# %%\n",
    "analyze_linguistic_relevance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def substitue2(texte, replacables, output):\n",
    "    df = pd.read_csv(replacables, sep=\"\\t\")\n",
    "    with open(output, \"a\", encoding=\"utf-8\") as f_w:\n",
    "        with open(texte, \"r\", encoding=\"utf-8\") as f_r:\n",
    "            for ligne in f_r.readlines():\n",
    "                contenu = str(ligne)\n",
    "                nlp_content = nlp(contenu)\n",
    "                for mot in nlp_content:\n",
    "                    match = df[df.iloc[:, 0] == mot]  # Filter by first column\n",
    "                    if not match.empty:\n",
    "                        f_w.write(str(match.iloc[0, 1])+ \" \")\n",
    "                    else:\n",
    "                        f_w.write(str(mot) + \" \")\n",
    "                f_w.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m substitue2(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD3/corpus_nettoyé2.XML\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/lemmes.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/corpus_post_lems2.XML\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36msubstitue2\u001b[1;34m(texte, replacables, output)\u001b[0m\n\u001b[0;32m      8\u001b[0m nlp_content \u001b[38;5;241m=\u001b[39m nlp(contenu)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mot \u001b[38;5;129;01min\u001b[39;00m nlp_content:\n\u001b[1;32m---> 10\u001b[0m     match \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m mot]  \u001b[38;5;66;03m# Filter by first column\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     12\u001b[0m         f_w\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(match\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ntich\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32mc:\\Users\\ntich\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmp_method(other, operator\u001b[38;5;241m.\u001b[39meq)\n",
      "File \u001b[1;32mc:\\Users\\ntich\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\ntich\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ntich\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:130\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mscalar_compare(x\u001b[38;5;241m.\u001b[39mravel(), y, op)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "substitue2(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD3/corpus_final.XML\",\n",
    "        \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/lemmes.csv\",\n",
    "        \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/corpus_post_final.XML\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def substitue(texte, replacables, output):\n",
    "    df = pd.read_csv(replacables, sep=\"\\t\")\n",
    "    with open(output, \"a\", encoding=\"utf-8\") as f_w:\n",
    "        with open(texte, \"r\", encoding=\"utf-8\") as f_r:\n",
    "            for ligne in f_r.readlines():\n",
    "                mots = str(ligne).split()\n",
    "                for mot in mots:\n",
    "                    match = df[df.iloc[:, 0] == mot]  # Filter by first column\n",
    "                    if not match.empty:\n",
    "                        f_w.write(str(match.iloc[0, 1])+ \" \")\n",
    "                    else:\n",
    "                        f_w.write(mot + \" \")\n",
    "                f_w.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitue(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD3/corpus_final.XML\",\n",
    "        \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/lemmes.csv\",\n",
    "        \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/corpus_post_final.XML\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contenu(texte):\n",
    "    texte = texte.lower()\n",
    "    \"fonction qui supprime les caractères inutiles\"\n",
    "    ponctuations = [\n",
    "    \".\", \",\", \"?\", \":\", \"\\n\", '\"', '!'\n",
    "    ]\n",
    "    for ponct in ponctuations:\n",
    "        while ponct in texte:\n",
    "            texte = texte.replace(ponct, \" \")\n",
    "\n",
    "    articles_fr = [\n",
    "        \"le\", \"la\", \"les\",        # Définis\n",
    "        \"un\", \"une\", \"des\",       # Indéfinis\n",
    "        \"du\", \"de la\", \"de l’\", \"des\",  # Partitifs\n",
    "        \"l’\", \"l'\",                    # Élision\n",
    "        \"au\", \"aux\", \"à la\", \"à l’\",  # Contractions avec \"à\"\n",
    "        \"du\", \"des\", \"de la\", \"de l’\",  # Contractions avec \"de\"\n",
    "        \"de\", \"d'\"\n",
    "    ]\n",
    "    for art in articles_fr:\n",
    "        texte = texte.replace(\" \" + art, \" \")\n",
    "    return texte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_index(fichier, output1, output2):\n",
    "            dict1 = {}\n",
    "            dict2 = {}\n",
    "            with open(fichier, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc_id = 0\n",
    "                for ligne in f.readlines():\n",
    "                    if \"<fichier>\" in ligne:\n",
    "                        doc_id = ligne[9:14]\n",
    "                    \n",
    "                    if \"<titre>\" in ligne:\n",
    "                        texte = ligne.replace(\"<titre>\", \"\")\n",
    "                        texte = texte.replace(\"</titre>\", \"\")\n",
    "                        if texte in dict2.keys():\n",
    "                            dict2[texte].append(doc_id)\n",
    "                        else: \n",
    "                            dict2[texte] = [doc_id] \n",
    "\n",
    "                        texte = clean_contenu(texte)\n",
    "                        mots = texte.split()\n",
    "                        #print(mots)\n",
    "                        for mot in mots:\n",
    "                            if mot in dict1.keys():\n",
    "                                 dict1[mot].append(doc_id)\n",
    "                            else: \n",
    "                                 dict1[mot] = [doc_id] \n",
    "                        \n",
    "                    if (\"<texte>\" in ligne):\n",
    "                        texte = str(ligne)\n",
    "                        texte = texte.replace(\"<texte>\", \"\")\n",
    "                        texte = texte.replace(\"</texte>\", \"\")\n",
    "                        \n",
    "                        texte = clean_contenu(texte)\n",
    "                        mots = texte.split()\n",
    "                        #print(mots)\n",
    "                        for mot in mots:\n",
    "                            if mot in dict1.keys():\n",
    "                                 dict1[mot].append(doc_id)\n",
    "                            else: \n",
    "                                 dict1[mot] = [doc_id] \n",
    "                    if (\"<rubrique>\" in ligne) or (\"<date>\" in ligne):\n",
    "                        texte = str(ligne)\n",
    "                        texte = texte.replace(\"<rubrique>\", \"\")\n",
    "                        texte = texte.replace(\"</rubrique>\", \"\")\n",
    "                        texte = texte.replace(\"<date>\", \"\")\n",
    "                        texte = texte.replace(\"</date>\", \"\")\n",
    "                        texte = texte.replace(\"\\n\", \"\")\n",
    "                        if texte in dict2.keys():\n",
    "                            dict2[texte].append(doc_id)\n",
    "                        else: \n",
    "                            dict2[texte] = [doc_id] \n",
    "            \n",
    "            with open(output1, \"w\", encoding=\"utf-8\") as f_w:\n",
    "                f_w.write(\"mot\\tdocs\\n\")\n",
    "                for key, tab in dict1.items():\n",
    "                    f_w.write(key + \"\\t\" + str(tab) + \"\\n\")\n",
    "            \n",
    "            with open(output2, \"w\", encoding=\"utf-8\") as f_w:\n",
    "                f_w.write(\"mot\\tdocs\\n\")\n",
    "                for key, tab in dict2.items():\n",
    "                    f_w.write(key + \"\\t\" + str(tab) + \"\\n\")\n",
    "                      \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_reverse_index(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/corpus_post_lems.XML\",\n",
    "                     \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/reverse_index_key.csv\",\n",
    "                     \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/reverse_index_date_rubrique.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_index_titres(corpus, output_titres):\n",
    "    dict = {}\n",
    "    with open(corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc_id = 0\n",
    "                for ligne in f.readlines():\n",
    "                    if \"<fichier>\" in ligne:\n",
    "                        doc_id = ligne[9:14]\n",
    "                    \n",
    "                    if \"<titre>\" in ligne:\n",
    "                        texte = ligne.replace(\"<titre>\", \"\")\n",
    "                        texte = texte.replace(\"</titre>\", \"\")\n",
    "\n",
    "                        texte = clean_contenu(texte)\n",
    "                        mots = texte.split()\n",
    "                        \n",
    "                        for mot in mots:\n",
    "                            if mot in dict.keys():\n",
    "                                 dict[mot].append(doc_id)\n",
    "                            else: \n",
    "                                 dict[mot] = [doc_id] \n",
    "\n",
    "    with open(output_titres, \"w\", encoding=\"utf-8\") as f_w:\n",
    "                f_w.write(\"mot\\tdocs\\n\")\n",
    "                for key, tab in dict.items():\n",
    "                    f_w.write(key + \"\\t\" + str(tab) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_reverse_index_titres(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/corpus_post_lems.XML\",\n",
    "                             \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/reverse_index_titre.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_index_texte(corpus, output_texte):\n",
    "    dict_texte = {}\n",
    "    with open(corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc_id = 0\n",
    "                for ligne in f.readlines():\n",
    "                    if \"<fichier>\" in ligne:\n",
    "                        doc_id = ligne[9:14]\n",
    "                    \n",
    "                    if \"<texte>\" in ligne:\n",
    "                        texte = ligne.replace(\"<texte>\", \"\")\n",
    "                        texte = texte.replace(\"</texte>\", \"\")\n",
    "\n",
    "                        texte = clean_contenu(texte)\n",
    "                        mots = texte.split()\n",
    "                        \n",
    "                        for mot in mots:\n",
    "                            if mot in dict_texte.keys():\n",
    "                                 dict_texte[mot].append(doc_id)\n",
    "                            else: \n",
    "                                 dict_texte[mot] = [doc_id] \n",
    "\n",
    "    with open(output_texte, \"w\", encoding=\"utf-8\") as f_w:\n",
    "                f_w.write(\"mot\\tdocs\\n\")\n",
    "                for key, tab in dict_texte.items():\n",
    "                    f_w.write(key + \"\\t\" + str(tab) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_reverse_index_texte(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/corpus_post_lems.XML\",\n",
    "                             \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/reverse_index_texte.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_index_date(corpus, output_date):\n",
    "    dict_date = {}\n",
    "    with open(corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc_id = 0\n",
    "                for ligne in f.readlines():\n",
    "                    if \"<fichier>\" in ligne:\n",
    "                        doc_id = ligne[9:14]\n",
    "                    \n",
    "                    if \"<date>\" in ligne:\n",
    "                        date = ligne.replace(\"<date>\", \"\")\n",
    "                        date = date.replace(\"</date>\", \"\")\n",
    "                        date = date.replace(\"\\n\", \"\")\n",
    "                        \n",
    "                        if date in dict_date.keys():\n",
    "                                 dict_date[date].append(doc_id)\n",
    "                        else: \n",
    "                                 dict_date[date] = [doc_id] \n",
    "\n",
    "    with open(output_date, \"w\", encoding=\"utf-8\") as f_w:\n",
    "                f_w.write(\"mot\\tdocs\\n\")\n",
    "                for key, tab in dict_date.items():\n",
    "                    f_w.write(key + \"\\t\" + str(tab) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_reverse_index_date(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/corpus_post_lems.XML\",\n",
    "                             \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/reverse_index_date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_index_rubrique(corpus, output_date):\n",
    "    dict_rubrique = {}\n",
    "    with open(corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc_id = 0\n",
    "                for ligne in f.readlines():\n",
    "                    if \"<fichier>\" in ligne:\n",
    "                        doc_id = ligne[9:14]\n",
    "                    \n",
    "                    if \"<rubrique>\" in ligne:\n",
    "                        rubrique = ligne.replace(\"<rubrique>\", \"\")\n",
    "                        rubrique = rubrique.replace(\"</rubrique>\", \"\")\n",
    "                        rubrique = rubrique.replace(\"\\n\", \"\")\n",
    "                        \n",
    "                        if rubrique in dict_rubrique.keys():\n",
    "                                 dict_rubrique[rubrique].append(doc_id)\n",
    "                        else: \n",
    "                                 dict_rubrique[rubrique] = [doc_id] \n",
    "\n",
    "    with open(output_date, \"w\", encoding=\"utf-8\") as f_w:\n",
    "                f_w.write(\"mot\\tdocs\\n\")\n",
    "                for key, tab in dict_rubrique.items():\n",
    "                    f_w.write(key + \"\\t\" + str(tab) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_reverse_index_rubrique(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/corpus_post_lems.XML\",\n",
    "                             \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/reverse_index_rubrique.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_index_images(corpus, output_image):\n",
    "    dict_image = {\"yes\": [], \"no\": []}\n",
    "    with open(corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc_id = 0\n",
    "                res = False\n",
    "                for ligne in f.readlines():\n",
    "                    if \"<bulletin>\" in ligne:\n",
    "                        res = False\n",
    "                    if \"<fichier>\" in ligne:\n",
    "                        doc_id = ligne[9:14]\n",
    "                    if \"<images>\" in ligne:\n",
    "                        res = True\n",
    "    \n",
    "                    if \"</bulletin>\" in ligne:\n",
    "                        if res == True:\n",
    "                            dict_image[\"yes\"].append(doc_id) \n",
    "                        else:\n",
    "                            dict_image[\"no\"].append(doc_id)\n",
    "\n",
    "    with open(output_image, \"w\", encoding=\"utf-8\") as f_w:\n",
    "                f_w.write(\"mot\\tdocs\\n\")\n",
    "                for key, tab in dict_image.items():\n",
    "                    f_w.write(key + \"\\t\" + str(tab) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_reverse_index_images(\"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/corpus_post_lems.XML\",\n",
    "                             \"C:/Users/ntich/OneDrive/Desktop/school shit/LO17/TD4/reverse_index_image.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
